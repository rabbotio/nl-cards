[
    {
        "front": {
            "title": "Model"
        },
        "back": {
            "desc": "The representation of what an ML system has learned from the training data.",
            "math": "\\text{Data} \\xrightarrow[ðŸ¤–]{\\text{train}} \\text{Model} \\xrightarrow[ðŸ¤–]{\\text{predict}} \\text{Prediction}",
            "bullets": [
                "Predictive model can then serve up predictions about previously unseen data.",
                "We use these predictions to take action in a product."
            ]
        },
        "res": [
            "[Framing : Model](https://developers.google.com/machine-learning/crash-course/framing/ml-terminology)"
        ]
    },
    {
        "front": {
            "title": "Accuracy"
        },
        "back": {
            "math": "\\text{Accuracy} = \\frac{TP+TN}{TP+TN+FP+FN}",
            "desc": "The fraction of predictions that a classification model got right.",
            "bullets": [
                "T = True",
                "F = False"
            ],
            "bullets_extra": [
                "P = Positives",
                "N = Negatives"
            ]
        },
        "res": []
    },
    {
        "front": {
            "title": "Precision"
        },
        "back": {
            "math": "\\text{Precision} = \\frac{TP}{TP+FP}",
            "desc": "A metric for classification models. Precision identifies the frequency with which a model was correct when predicting the positive class.",
            "bullets": [
                "T = True",
                "F = False"
            ],
            "bullets_extra": [
                "P = Positives",
                "N = Negatives"
            ]
        },
        "res": []
    },
    {
        "front": {
            "title": "Recall"
        },
        "back": {
            "math": "\\text{Recall} = \\frac{\\text{TP}} {\\text{TP} + \\text{FN}}",
            "desc": "A metric for classification models that answers the following question: Out of all the possible positive labels, how many did the model correctly identify?",
            "bullets": [
                "T = True",
                "F = False"
            ],
            "bullets_extra": [
                "P = Positives",
                "N = Negatives"
            ]
        },
        "res": []
    },
    {
        "front": {
            "title": "L<sub>2</sub> Regularization"
        },
        "back": {
            "desc": "A type of regularization that penalizes weights in proportion to the sum of the squares of the weights.",
            "math": "L_2\\text{ Regularization} = ||\\boldsymbol w||_2^2 = {w_1^2 + w_2^2 + ... + w_n^2}",
            "bullets": [
                "Drive outlier weights (high positive or low negative values) closer to 0 but not quite to 0.",
                "Always improves generalization in linear models."
            ]
        },
        "res": []
    },
    {
        "front": {
            "title": "Supervised Machine Learning"
        },
        "back": {
            "desc": "Training a <b>model</b> from input data and its corresponding labels. Provide answers for never-before-seen questions on the same topic",
            "math": "\\text{Labeled Data} \\xrightarrow[\\text{features x}]{\\text{labels y}} \\text{Predictive Model}",
            "bullets": [
                "<dfn>\\[x\\]</dfn> is feature.<br/>",
                "<dfn>\\[y\\]</dfn> is label."
            ]
        },
        "res": [
            "[Supervised Machine Learning](https://developers.google.com/machine-learning/glossary/#supervised_machine_learning)"
        ]
    },
    {
        "front": {
            "title": "Linear Regression",
            "image": "./ml/img/supervised.svg"
        },
        "back": {
            "desc": "A type of regression model that outputs a continuous value from a linear combination of input features.",
            "math": "y' = b + w_1x_1",
            "bullets": [
                "<dfn>\\[y'\\]</dfn> is the predicted label (a desired output).",
                "<dfn>\\[b\\]</dfn> (or <dfn>\\[w_0\\]</dfn>) is the bias (the y-intercept).",
                "<dfn>\\[w_1\\]</dfn> is the weight (slope) of feature 1.",
                "<dfn>\\[x_1\\]</dfn>  is a feature (a known input)."
            ]
        },
        "res": [
            "[Descending into ML: Linear Regression](https://developers.google.com/machine-learning/crash-course/descending-into-ml/linear-regression)"
        ]
    },
    {
        "front": {
            "title": "Training"
        },
        "back": {
            "desc": "<b>Training</b> a model simply means learning (determining) good values for all the weights and the bias from labeled examples.",
            "math": "\\text{Labeled Data} \\xrightarrow[\\text{features x}]{\\text{labels y}} \\text{Predictive Model}",
            "bullets": [
                "In supervised learning, a machine learning algorithm builds a model by examining many examples and attempting to find a model that minimizes loss.",
                "The goal of training a model is to find a set of weights and biases that have low loss, on average, across all examples."
            ]
        },
        "res": [
            "[Descending into ML: Training and Loss](https://developers.google.com/machine-learning/crash-course/descending-into-ml/training-and-loss)"
        ]
    },
    {
        "front": {
            "title": "Loss"
        },
        "back": {
            "desc": "<b>Loss</b> is the penalty for a bad prediction. That is, <b>loss</b> is a number indicating how bad the model's prediction was on a single example. ",
            "image": "./ml/img/loss.png",
            "bullets": [
                "If the model's prediction is perfect, the loss is zero; otherwise, the loss is greater.",
                "Loss function is a mathematical function that would aggregate the individual losses in a meaningful fashion."
            ]
        },
        "res": [
            "[Descending into ML: Training and Loss](https://developers.google.com/machine-learning/crash-course/descending-into-ml/training-and-loss)"
        ]
    },
    {
        "front": {
            "title": "L<sub>2</sub> Loss"
        },
        "back": {
            "desc": "The <b>loss function</b> calculates the squares of the difference between a model's predicted value for a labeled example and the actual value of the label.",
            "math": "\\begin{align}L_2 Loss &= (observation - prediction(x))^2\\\\&= (y - y')^2 \\end{align}",
            "bullets": [
                "Used in linear regression.",
                "Due to squaring, this loss function amplifies the influence of bad predictions.",
                "Squared loss reacts more strongly to outliers than <dfn>\\[L_1\\]</dfn> Loss."
            ]
        },
        "res": [
            "[Descending into ML: Training and Loss](https://developers.google.com/machine-learning/crash-course/descending-into-ml/training-and-loss)"
        ]
    },
    {
        "front": {
            "title": "Mean Square Error (MSE)"
        },
        "back": {
            "desc": "<b>Mean Square Error (MSE)</b> is the average squared loss per example over the whole dataset.",
            "math": "MSE = \\frac{1}{N} \\sum_{(x,y)\\in D} (y - prediction(x))^2",
            "bullets": [
                "<dfn>\\[x\\]</dfn> is the set of features, <dfn>\\[y\\]</dfn> is the example's label",
                "<dfn>\\[prediction(x)\\]</dfn> is a function of the weights and bias in combination with the set of features <dfn>\\[x\\]</dfn>",
                "<dfn>\\[D\\]</dfn> is a data set containing many labeled examples, which are <dfn>\\[(x,y)\\]</dfn>.",
                "<dfn>\\[N\\]</dfn> is the number of examples in <dfn>\\[D\\]</dfn>."
            ]
        },
        "res": [
            "[Descending into ML: Mean Square Error (MSE)](https://developers.google.com/machine-learning/crash-course/descending-into-ml/training-and-loss)"
        ]
    },
    {
        "front": {
            "title": "Gradient Descent"
        },
        "back": {
            "desc": "A technique to minimize loss by computing the gradients of loss with respect to the model's parameters, conditioned on training data.",
            "image": "./ml/img/gradient_descent.svg",
            "bullets": [
                "Informally, gradient descent iteratively adjusts parameters, gradually finding the best combination of weights and bias to minimize loss."
            ]
        },
        "res": [
            "[Reducing Loss: Gradient Descent](https://developers.google.com/machine-learning/crash-course/reducing-loss/gradient-descent)"
        ]
    },
    {
        "front": {
            "title": "Learning Rate"
        },
        "back": {
            "desc": "Gradient descent algorithms multiply the gradient by a scalar known as the <b>learning rate</b> to determine the next point.",
            "image": "./ml/img/learning_rate.svg",
            "bullets": [
                "Also know as <b>step size</b>",
                "Learning rate is a key <b>hyperparameter</b>."
            ]
        },
        "res": [
            "[Reducing Loss: Learning Rate](https://developers.google.com/machine-learning/crash-course/reducing-loss/learning-rate)",
            "[Reducing Loss: Optimizing Learning Rate](https://developers.google.com/machine-learning/crash-course/fitter/graph)"
        ]
    },
    {
        "front": {
            "title": "Stochastic Gradient Descent"
        },
        "back": {
            "desc": "A gradient descent algorithm in which the batch size is one. In other words, SGD relies on a single example chosen uniformly at random from a dataset to calculate an estimate of the gradient at each step."
        }
    },
    {
        "front": {
            "title": "Overfitting"
        },
        "back": {
            "desc": "Creating a model that matches the training data so closely that the model fails to make correct predictions on new data."
        },
        "res": [
            "[Generalization: Peril of Overfitting](https://developers.google.com/machine-learning/crash-course/generalization/peril-of-overfitting)"
        ]
    },
    {
        "front": {
            "title": "Training and Test Sets"
        },
        "back": {
            "desc": "Make sure that your test set is large enough to yield statistically meaningful results and representative of the data set as a whole.",
            "image": "./ml/img/training_n_test_set.svg",
            "bullets": [
                "Training set = a subset to train a model.",
                "Test set = a subset to test the trained model.",
                "Don't pick a different characteristics test set than the training set.",
                "Never train on test data."
            ]
        },
        "res": [
            "[Reducing Loss: Learning Rate](https://developers.google.com/machine-learning/crash-course/reducing-loss/learning-rate)",
            "[Reducing Loss: Optimizing Learning Rate](https://developers.google.com/machine-learning/crash-course/fitter/graph)"
        ]
    }
]